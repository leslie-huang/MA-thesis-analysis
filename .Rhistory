devtools::install_github("quanteda", username="kbenoit", dependencies=TRUE, ref="dev")
install.packages("quanteda")
install.packages("quantedaData")
install.packages("devtools")
library(devtools)
## be sure to install the latest version from GitHub, using dev branch:
devtools::install_github("quanteda", username="kbenoit", dependencies=TRUE, ref="dev")
## and quantedaData
devtools::install_github("quantedaData", username="kbenoit")
library(quanteda)
library(quantedaData)
sampletxt <- "The police with their policing strategy instituted a policy of general
iterations at the Data Science Institute."
#Let's tokenize
tokens<-tokenize(sampletxt)
?tokenize
tokens
tokens<-tokenize(sampletxt, removePunct=TRUE)
tokens
stems<-wordstem(tokens)
stems
data("SOTUCorpus")
head(SOTUCorpus)
speeches<-data.frame(SOTUCorpus$documents)
speeches
last_speech_text<-speeches$texts[230]
last_speech_text
?dfm
class(speeches$texts)
obama_dfm<-dfm(last_speech_text)
?stopwords
stopwords("english")
obama_dfm<-dfm(last_speech_text, ignoredFeatures = stopwords("english"))
?topfeatures
full_dfm<-dfm(speeches$texts, ignoredFeatures = stopwords("english"))
topfeatures(full_dfm)
?tfidf
weighted<-tfidf(full_dfm)
topfeatures(weighted)
normalized<-tfidf(full_dfm, normalize=TRUE)
topfeatures(normalized)
collocations(last_speech_text)
?collocations
collocations(last_speech_text, size=3)
colloc <- collocations(last_speech_text)
colloc
isStopwordList <- lapply(colloc$word1, `%in%`, stopwords("english"))
stopwordindex <- which(colloc$word1 %in% stopwords("english")| colloc$word2 %in% stopwords("english"))
colloc[-stopwordindex]  # collocations not containing stopwords
stopwordindex <- which(colloc$word1 %in% stopwords("english")| colloc$word2 %in% stopwords("english"))
colloc[-stopwordindex]  # collocations not containing stopwords
install.packages(plotly)
install.packages("plotly")
library(quanteda)
library(quantedaData)
##load data
data(inaugCorpus)
tokens<-tokenize(inaugCorpus, removePunct=TRUE)
Tee<-lapply(tokens,  length )
Tee<-sum(unlist(Tee))
tokens
Tee
mydfm <- dfm(inaugCorpus)
M<-length(mydfm@Dimnames$features)
M
k<- 44
b<-.49
k * (Tee)^b
inaugCorpus$document$texts[1]
inaugCorpus$document$texts[57]
mydfm <- dfm(inaugCorpus)
plot(log10(1:100), log10(topfeatures(mydfm, 100)),
xlab="log10(rank)", ylab="log10(frequency)", main="Top 100 Words")
regression <- lm(log10(topfeatures(mydfm, 100)) ~ log10(1:100))
abline(regression, col="red")
confint(regression)
mydfm <- dfm(inaugCorpus, ignoredFeatures=stopwords("english"))
plot(log10(1:100), log10(topfeatures(mydfm, 100)),
xlab="log10(rank)", ylab="log10(frequency)", main="Top 100 Words")
# regression to check if slope is approx -1.0
regression <- lm(log10(topfeatures(mydfm, 100)) ~ log10(1:100))
abline(regression, col="red")
confint(regression)
topfeatures(mydfm, 20)
last_speech_text<-inaugCorpus$document$texts[57]
last_speech_text
collocations(last_speech_text)
collocations(last_speech_text, size=3)
colloc <- collocations(last_speech_text)
colloc
isStopwordList <- lapply(colloc$word1, `%in%`, stopwords("english"))
stopwordindex <- which(colloc$word1 %in% stopwords("english")| colloc$word2 %in% stopwords("english"))
colloc[-stopwordindex]  # collocations not containing stopwords
kwic(inaugCorpus, "terror", 3)
kwic(inaugCorpus, "angels", 3)
kwic(inaugCorpus, "slavery", 3)
x<-c(1,2,3)
y<-c(1,2,3)
##define the norm
norm_vec <- function(x) sqrt(sum(x^2))
norm_vec
norm_vec(x)
x %*% y / (norm_vec(x)*norm_vec(y))
a<-c(1,2,3)
b<-c(1,2,4000)
a %*% b / (norm_vec(a)*norm_vec(b))
last_speech_text<-inaugCorpus$document$texts[57]
first_speech_text<-inaugCorpus$document$texts[1]
inaug_dfm<-dfm(c(last_speech_text, first_speech_text),ignoredFeatures = stopwords("english"),    stem = TRUE)
tmp <- similarity(inaug_dfm, margin = "documents")
tmp
as.matrix(tmp)
inaug_dfm<-dfm(c(last_speech_text, first_speech_text))
#calculate similarity
tmp <- similarity(inaug_dfm, margin = "documents")
as.matrix(tmp)
inaug_dfm<-dfm(subset(inaugCorpus , Year > 1980),ignoredFeatures = stopwords("english"),    stem = TRUE)
tmp <- similarity(inaug_dfm, margin = "documents")
as.matrix(tmp)
similarity(inaug_dfm, "2009-Obama", n = 5, margin = "documents")
install.packages("zelig")
install.packages("Zelig")
install.packages("MatchIt")
install.packages("cem")
install.packages(c("curl", "NLP", "topicmodels"))
# Quant 2
# Leslie Huang
# PS 4
# Set up the workspace and libraries
setwd("/Users/lesliehuang/Dropbox/PS4")
library(foreign)
library(stargazer)
library(Zelig)
library(MatchIt)
library(cem)
# Import data (already cleaned)
trcdata <- read.dta("trckeep.dta")
install.packages("Zelig")
install.packages("MatchIt")
install.packages("MatchIt")
install.packags("cem")
install.packages("cem")
# Quant 2
# Leslie Huang
# PS 4
# Set up the workspace and libraries
setwd("/Users/lesliehuang/Dropbox/PS4")
library(foreign)
library(stargazer)
library(Zelig)
library(MatchIt)
library(cem)
# Import data (already cleaned)
trcdata <- read.dta("trckeep.dta")
library(quanteda)
library(quantedaData)
##load data
##load in data
data("iebudgetsCorpus")
df<-data.frame(iebudgetsCorpus$documents)
# Quant 2
# Leslie Huang
# PS 4
# Set up the workspace and libraries
setwd("/Users/lesliehuang/Dropbox/PS4")
library(foreign)
library(stargazer)
library(MatchIt)
# Import data (already cleaned)
trcdata <- read.dta("trckeep.dta")
setwd("/Users/lesliehuang/Dropbox/PS4/")
setwd("Users/lesliehuang/Dropbox/PS4")
tokens<-tokenize(iebudgetsCorpus, removePunct=TRUE)
class(df)
str(df)
class(tokens)
tokenz<-lapply(tokens,  length )
tokens
tokens[1]
tokenz[1]
library(MatchIt)
library(Zelig)
install.packages("graph")
typez<-lapply(lapply(tokens,  unique ), length)
typez[1]
library(quanteda)
library(quantedaData)
##load data
##load in data
data("iebudgetsCorpus")
df<-data.frame(iebudgetsCorpus$documents)
## Lexical diversity measures
# TTR
tokens<-tokenize(iebudgetsCorpus, removePunct=TRUE)
tokenz<-lapply(tokens,  length )
typez<-lapply(lapply(tokens,  unique ), length)
TTRz<-mapply("/",typez,tokenz,SIMPLIFY = FALSE)
TTRz
df$ttr<-unlist(TTRz)
str(df)
plot(df$ttr)
library("Zelig", lib.loc="/Library/Frameworks/R.framework/Versions/3.2/Resources/library")
remove.packages("Zelig")
remove.packages("cem")
df$year<-as.numeric(df$year)
aggregate(df$ttr, by=list(df$year), FUN=mean)
table(df$year)
aggregate(df$ttr, by=list(df$party), FUN=mean)
table(df$party)
?readability
df$read_FRE<-readability(df$texts, "Flesch")
aggregate(df$read_FRE, by=list(df$year), FUN=mean)
aggregate(df$read_FRE, by=list(df$party), FUN=mean)
readability(df$texts, "Fucks")
df$read_FRE<-readability(df$texts, "Flesch")
df$read_FRE
aggregate(df$read_FRE, by=list(df$year), FUN=mean)
aggregate(df$read_FRE, by=list(df$party), FUN=mean)
df$read_DC<-readability(df$texts, "Dale.Chall")
aggregate(df$read_DC, by=list(df$year), FUN=mean)
aggregate(df$read_DC, by=list(df$party), FUN=mean)
read<-readability(df$texts)
cor(read$Flesch, read$Dale.Chall)
TTRz
cor(read$Flesch, read$SMOG)
cor(read$Coleman.Liau, read$Dale.Chall)
cor(read$Fucks, read$Dale.Chall)
install.packages("dplyr")
install.packages("dplyr")
library(dplyr)
year_FRE<-data.frame(matrix(ncol = 5, nrow = 100))
df<-filter(df, party != "WUAG" & party != "SOC"  & party != "PBPA" )
party_FRE<-data.frame(matrix(ncol = 6, nrow = 100))
for(i in 1:100){
#sample 200
bootstrapped<-sample_n(df, 200, replace=TRUE)
bootstrapped$read_FRE<-readability(bootstrapped$texts, "Flesch")
#store results
year_FRE[i,]<-aggregate(bootstrapped$read_FRE, by=list(bootstrapped$year), FUN=mean)[,2]
party_FRE[i,]<-aggregate(bootstrapped$read_FRE, by=list(bootstrapped$party), FUN=mean)[,2]
}
library(quanteda)
for(i in 1:100){
#sample 200
bootstrapped<-sample_n(df, 200, replace=TRUE)
bootstrapped$read_FRE<-readability(bootstrapped$texts, "Flesch")
#store results
year_FRE[i,]<-aggregate(bootstrapped$read_FRE, by=list(bootstrapped$year), FUN=mean)[,2]
party_FRE[i,]<-aggregate(bootstrapped$read_FRE, by=list(bootstrapped$party), FUN=mean)[,2]
}
colnames(year_FRE)<-names(table(df$year))
colnames(party_FRE)<-names(table(df$party))
std <- function(x) sd(x)/sqrt(length(x))
year_ses<-apply(year_FRE, 2, std)
year_means<-apply(year_FRE, 2, mean)
party_ses<-apply(party_FRE, 2, std)
party_means<-apply(party_FRE, 2, mean)
###Plot results--year
coefs<-year_means
ses<-year_ses
y.axis <- c(1:5)
min <- min(coefs - 2*ses)
max <- max(coefs + 2*ses)
var.names <- colnames(year_FRE)
adjust <- 0
par(mar=c(2,8,2,2))
plot(coefs, y.axis, type = "p", axes = F, xlab = "", ylab = "", pch = 19, cex = .8,
xlim=c(min,max),ylim = c(.5,6.5), main = "")
rect(min,.5,max,1.5, col = c("grey97"), border="grey90", lty = 2)
rect(min,1.5,max,2.5, col = c("grey95"), border="grey90", lty = 2)
rect(min,2.5,max,3.5, col = c("grey97"), border="grey90", lty = 2)
rect(min,3.5,max,4.5, col = c("grey95"), border="grey90", lty = 2)
rect(min,4.5,max,5.5, col = c("grey97"), border="grey90", lty = 2)
#rect(min,5.5,max,6.5, col = c("grey97"), border="grey90", lty = 2)
axis(1, at = seq(min,max,(max-min)/10),
labels = c(round(min+0*((max-min)/10),3),
round(min+1*((max-min)/10),3),
round(min+2*((max-min)/10),3),
round(min+3*((max-min)/10),3),
round(min+4*((max-min)/10),3),
round(min+5*((max-min)/10),3),
round(min+6*((max-min)/10),3),
round(min+7*((max-min)/10),3),
round(min+8*((max-min)/10),3),
round(min+9*((max-min)/10),3),
round(max,3)),tick = T,cex.axis = .75, mgp = c(2,.7,0))
axis(2, at = y.axis, label = var.names, las = 1, tick = FALSE, cex.axis =.8)
abline(h = y.axis, lty = 2, lwd = .5, col = "white")
segments(coefs-qnorm(.975)*ses, y.axis+2*adjust, coefs+qnorm(.975)*ses, y.axis+2*adjust, lwd =  1)
segments(coefs-qnorm(.95)*ses, y.axis+2*adjust-.035, coefs-qnorm(.95)*ses, y.axis+2*adjust+.035, lwd = .9)
segments(coefs+qnorm(.95)*ses, y.axis+2*adjust-.035, coefs+qnorm(.95)*ses, y.axis+2*adjust+.035, lwd = .9)
points(coefs, y.axis+2*adjust,pch=21,cex=.8, bg="white")
library(ggplot2)
plot(coefs, y.axis, type = "p", axes = F, xlab = "", ylab = "", pch = 19, cex = .8,
xlim=c(min,max),ylim = c(.5,6.5), main = "")
rect(min,.5,max,1.5, col = c("grey97"), border="grey90", lty = 2)
rect(min,1.5,max,2.5, col = c("grey95"), border="grey90", lty = 2)
rect(min,2.5,max,3.5, col = c("grey97"), border="grey90", lty = 2)
rect(min,3.5,max,4.5, col = c("grey95"), border="grey90", lty = 2)
rect(min,4.5,max,5.5, col = c("grey97"), border="grey90", lty = 2)
#rect(min,5.5,max,6.5, col = c("grey97"), border="grey90", lty = 2)
axis(1, at = seq(min,max,(max-min)/10),
labels = c(round(min+0*((max-min)/10),3),
round(min+1*((max-min)/10),3),
round(min+2*((max-min)/10),3),
round(min+3*((max-min)/10),3),
round(min+4*((max-min)/10),3),
round(min+5*((max-min)/10),3),
round(min+6*((max-min)/10),3),
round(min+7*((max-min)/10),3),
round(min+8*((max-min)/10),3),
round(min+9*((max-min)/10),3),
round(max,3)),tick = T,cex.axis = .75, mgp = c(2,.7,0))
axis(2, at = y.axis, label = var.names, las = 1, tick = FALSE, cex.axis =.8)
abline(h = y.axis, lty = 2, lwd = .5, col = "white")
segments(coefs-qnorm(.975)*ses, y.axis+2*adjust, coefs+qnorm(.975)*ses, y.axis+2*adjust, lwd =  1)
segments(coefs-qnorm(.95)*ses, y.axis+2*adjust-.035, coefs-qnorm(.95)*ses, y.axis+2*adjust+.035, lwd = .9)
segments(coefs+qnorm(.95)*ses, y.axis+2*adjust-.035, coefs+qnorm(.95)*ses, y.axis+2*adjust+.035, lwd = .9)
points(coefs, y.axis+2*adjust,pch=21,cex=.8, bg="white")
table(df$year)
aggregate(df$read_FRE, by=list(df$year), FUN=mean)
coefs<-party_means
ses<-party_ses
y.axis <- c(1:6)
min <- min(coefs - 2*ses)
max <- max(coefs + 2*ses)
var.names <- colnames(party_FRE)
adjust <- 0
par(mar=c(2,8,2,2))
plot(coefs, y.axis, type = "p", axes = F, xlab = "", ylab = "", pch = 19, cex = .8,
xlim=c(min,max),ylim = c(.5,6.5), main = "")
rect(min,.5,max,1.5, col = c("grey97"), border="grey90", lty = 2)
rect(min,1.5,max,2.5, col = c("grey95"), border="grey90", lty = 2)
rect(min,2.5,max,3.5, col = c("grey97"), border="grey90", lty = 2)
rect(min,3.5,max,4.5, col = c("grey95"), border="grey90", lty = 2)
rect(min,4.5,max,5.5, col = c("grey97"), border="grey90", lty = 2)
rect(min,5.5,max,6.5, col = c("grey97"), border="grey90", lty = 2)
axis(1, at = seq(min,max,(max-min)/10),
labels = c(round(min+0*((max-min)/10),3),
round(min+1*((max-min)/10),3),
round(min+2*((max-min)/10),3),
round(min+3*((max-min)/10),3),
round(min+4*((max-min)/10),3),
round(min+5*((max-min)/10),3),
round(min+6*((max-min)/10),3),
round(min+7*((max-min)/10),3),
round(min+8*((max-min)/10),3),
round(min+9*((max-min)/10),3),
round(max,3)),tick = T,cex.axis = .75, mgp = c(2,.7,0))
axis(2, at = y.axis, label = var.names, las = 1, tick = FALSE, cex.axis =.8)
abline(h = y.axis, lty = 2, lwd = .5, col = "white")
segments(coefs-qnorm(.975)*ses, y.axis+2*adjust, coefs+qnorm(.975)*ses, y.axis+2*adjust, lwd =  1)
segments(coefs-qnorm(.95)*ses, y.axis+2*adjust-.035, coefs-qnorm(.95)*ses, y.axis+2*adjust+.035, lwd = .9)
segments(coefs+qnorm(.95)*ses, y.axis+2*adjust-.035, coefs+qnorm(.95)*ses, y.axis+2*adjust+.035, lwd = .9)
points(coefs, y.axis+2*adjust,pch=21,cex=.8, bg="white")
##real world data
table(df$party)
aggregate(df$read_FRE, by=list(df$party), FUN=mean)
df$
browseVignettes(package = "dplyr")
0.1*9
View(table_for_paper_F)
rm(list=ls())
setwd("/Users/lesliehuang/Dropbox/MA-thesis-analysis/")
set.seed(1234)
libraries <- c("foreign", "utils", "stargazer", "dplyr", "devtools", "quanteda", "ggplot2", "stringr", "LIWCalike", "austin", "forecast", "lmtest", "strucchange", "vars", "tseries", "urca", "depmixS4", "rrcov", "mlogit", "reshape2", "MCDM", "car")
lapply(libraries, require, character.only=TRUE)
devtools::install_github("ggbiplot", "vqv")
library(ggbiplot)
# get LIWC dict
spanish_dict <- dictionary(file = "../LIWC/Spanish_LIWC2007_Dictionary.dic", format = "LIWC")
##################################################################################
##################################################################################
# get monthly levels of violence
monthly_viol <- read.csv("../MA-datasets/violence_stats.csv", stringsAsFactors = FALSE)
monthly_viol$date <- as.Date(as.yearmon(monthly_viol$date, "%Y-%m"))
monthly_viol[,2:4] <- sapply(monthly_viol[,2:4], function(x) { as.numeric(x)})
monthly_viol <- subset(monthly_viol, select = c(2:4, 1))
# some major dates for plotting
major_violence <- as.Date(c("7/20/13", "1/16/13", "7/29/14", "11/16/14", "4/15/15", "5/31/15", "6/15/15", "6/22/15"), "%m/%d/%y")
major_agree <- as.Date(c("8/26/12", "5/26/13", "11/6/13", "5/16/14", "3/7/15", "6/2/15", "9/23/15"), "%m/%d/%y")
cf_start <- as.Date(c("11/20/12", "12/15/13", "5/16/14", "12/20/14", "7/20/15"), "%m/%d/%y")
cf_end <- as.Date(c("1/20/13", "1/15/14", "5/28/14", "5/22/15", "1/1/16"), "%m/%d/%y")
ceasefires <- data.frame(start = as.Date(c("11/20/12", "12/15/13", "5/16/14", "12/20/14", "7/20/15"), "%m/%d/%y"), end = as.Date(c("1/20/13", "1/15/14", "5/28/14", "5/22/15", "1/1/16"), "%m/%d/%y"))
# df of all dates
dates <- rbind(data.frame(date = major_violence, group = "major_viol"), data.frame(date = major_agree, group = "major_agree"), data.frame(date = cf_start, group = "ceasefire_start"), data.frame(date = cf_end, group = "ceasefire_end"))
dates <- arrange(dates, date)
stargazer(dates, title="Labeling states using major events", digits = 2, digit.separator = "", summary = FALSE)
##################################################################################
# Some functions to extract sentiment, loess it, and return results
# Function to get raw LIWC measures
liwc_extractor <- function(df) {
# run liwc
liwc_results <- liwcalike(df$text, spanish_dict)
# get date metadata
df_dates <- dplyr::select(df, date)
date <- as.Date(df_dates[[1]], "%Y-%m-%d")
# extract the measures we want
liwc_results <- dplyr::select(liwc_results, EmoNeg, EmoPos, Ellos, Muerte)
# make the dataframe
results_df <- data.frame(cbind(sapply(liwc_results, function(x) {as.numeric(x)})))
results_df$date <- as.Date(date, "%Y-%m-%d")
return(results_df)
}
# Function to take a df of raw LIWC values and return loessed values
liwc_loess <- function(liwc_results) {
date <- as.Date(liwc_results$date, origin = "1970-01-01")
results_df <- data.frame(cbind(sapply(liwc_results[,1:4], function(x) { loess(x ~ as.numeric(liwc_results$date), control = loess.control(surface = "direct"))$y})))
results_df$date <- date
return(results_df)
}
# Function to return loess predictions as a list
loess_lines <- function(liwc_results) {
list_models <- list(sapply(liwc_results[,1:4], function(x) {loess(x ~ as.numeric(liwc_results$date), control=loess.control(surface="direct"))}))
return(list_models)
}
#################################################################################
# import FARC communiques
FARC <- read.csv("../MA-datasets/FARC_communiques.csv", stringsAsFactors = FALSE)
# fill in a document that didn't get scraped
FARC[120, 1] <- "2016-05-30"
FARC[120, 2] <- "Informamos sobre la muerte del Camarada Bernardo Peñaloza"
FARC[120, 3] <- ""
FARC[120, 4] <- "COMUNICADO: Mayo 30 de 2016 Lamentamos informar a toda la guerrillerada y a la opinión pública nacional e internacional, que el día 25 de Mayo el camarada Bernardo Peñaloza, ex integrante de la Comisión de Paz en los diálogos de la Habana, tras una larga y fructífera vida entregada a la lucha por la revolución y los cambios sociales, en territorio colombiano y en cumplimiento de sus tareas, falleció como consecuencia de un paro cardiaco.Expresamos nuestras condolencias a sus familiares y amigos que lo conocieron. Rendimos homenaje a su memoria y a su ejemplo de dedicación y lealtad a la causa de los oprimidos. Paz en su tumba. Estado Mayor Central de las FARC EP."
# exclude documents prior to 9/2012
FARC$date <- as.Date(FARC$date, "%Y-%m-%d")
FARC <- filter(FARC, date >= "2012-09-01")
# raw LIWC measures
FARC_raw <- liwc_extractor(FARC)
# loess it
FARC_results <- liwc_loess(FARC_raw)
# get the loess lines for plotting
FARC_lines <- loess_lines(FARC_raw)
#################################################################################
# do the same for joint communiques
joint <- read.csv("../MA-datasets/jointstatements.csv", stringsAsFactors = FALSE)
joint[18, 2] <- "Las delegaciones del Gobierno y las Farc-ep informan que la Mesa de Conversaciones retomará las sesiones de trabajo el próximo viernes 3 de julio para continuar discutiendo el tema de reparación que hace parte del Punto 5 de la Agenda “Víctimas”."
joint[20, 2] <- "Las delegaciones del Gobierno y las FARC-EP, en el marco del “ACUERDO SOBRE LIMPIEZA Y DESCONTAMINACIÓN DEL TERRITORIO DE LA PRESENCIA DE MINAS ANTIPERSONAL (MAP), ARTEFACTOS EXPLOSIVOS IMPROVISADOS (AEI) Y MUNICIONES SIN EXPLOTAR (MUSE) O RESTOS EXPLOSIVOS DE GUERRA (REG) EN GENERAL”, anunciado de manera conjunta el pasado 7 de marzo de 2015, se permiten informar los siguientes avances: 1. Como es de conocimiento público, las delegaciones seleccionaron la vereda El Orejón ubicada en el municipio de Briceño, Antioquia, para iniciar el proyecto piloto de desminado. 2. Hasta la zona viajó el grupo de gestión conformado por: delegados del Gobierno (Dirección para la Acción Integral contra Minas Antipersonal y el Batallón Humanitario de Desminado del Ejército Nacional), miembros representantes de las FARC-EP, técnicos de la organización Ayuda Popular Noruega (APN) con el acompañamiento de Cuba y Noruega, países garantes; y del Comité Internacional de la Cruz Roja (CICR). 3. Luego de la selección de sitios, se inició la segunda fase del proyecto denominada Estudio No Técnico (ENT) con el fin de recopilar la información para identificar las áreas realmente contaminadas por minas antipersonal (MAP), artefactos explosivos improvisados (AEI) y municiones sin explotar (MUSE) o restos explosivos de guerra (REG). Un equipo liderado por la Ayuda Popular Noruega (APN) entró en contacto con las comunidades para realizar entrevistas con el fin de recopilar información y socializar el proyecto. 4. El Estudio No Técnico (ENT) tuvo una duración de cerca de 7 días en campo. Pese a la complejidad del terreno,  las condiciones climáticas y la naturaleza del proyecto se registraron resultados satisfactorios. 5. Como resultado de un trabajo coordinado entre el Gobierno y las FARC-EP, se identificaron 4 áreas peligrosas que suman aproximadamente 12 mil metros cuadrados. El trabajo de limpieza de estas áreas contaminadas que serán despejadas como producto del Acuerdo, facilitarán la movilidad de la comunidad en riesgo y permitirán la restauración de derechos de las comunidades en términos de movilidad, esparcimiento, acceso a vías terrestres y uso productivo de la tierra. 6. Durante el Estudio No Técnico (ENT) se recogió la información precisa y útil para la siguiente fase de “limpieza y descontaminación”. 7. Como quedó establecido en el Acuerdo, el compromiso es mantener las áreas intervenidas libres de minas y artefactos explosivos, como una garantía de no repetición para el beneficio de las comunidades. 8. Agradecemos a la Ayuda Popular Noruega (APN), a Cuba y Noruega, países garantes; al Comité Internacional de la Cruz Roja (CICR), la Gobernación de Antioquia y a las autoridades locales por su apoyo y colaboración. 9. Queremos extender un especial agradecimiento a la comunidad de la vereda El Orejón por su disposición, respaldo y conciencia al adoptar comportamientos seguros ante el riesgo de las minas antipersonal. 10. Confiamos en que esta primera medida conjunta de desescalamiento permita en un tiempo prudencial llevar alivio a las comunidades más afectadas por el conflicto y avanzar hacia la solución de este."
# LIWC estimates
joint_raw <- liwc_extractor(joint)
# loessed point estimates
joint_results <- liwc_loess(joint_raw)
# get the loess lines for plotting
joint_lines <- loess_lines(joint_raw)
table_for_paper_F <- cbind(FARC[,-1], FARC_results)
table_for_paper_F <- dplyr::select(table_for_paper_F, date, text, EmoNeg, EmoPos)
colnames(table_for_paper_F) <- c("Date", "Text", "Percent neg. emotion", "Percent pos. emotion")
table_for_paper_g <- cbind(govt, govt_results[,-5])
table_for_paper_g <- dplyr::select(table_for_paper_g, date, text, EmoNeg, EmoPos)
table_for_paper_g$Translation <- NA
table_for_paper_g <- table_for_paper_g[,c("date", "text", "Translation", "EmoNeg", "EmoPos")]
colnames(table_for_paper_g) <- c("Date", "Text", "Translation", "Neg", "Pos")
# pick a few that we want
table_for_paper_g_n <- filter(table_for_paper_g, Neg > 28)
table_for_paper_g_n <- arrange(table_for_paper_g_n, desc(Neg))
table_for_paper_g_p <- filter(table_for_paper_g, Pos > 33)
table_for_paper_g_p <- arrange(table_for_paper_g_p, desc(Pos))
#################################################################################
# and the same for govt statements
govt <- read.csv("govtstatements.csv", stringsAsFactors = FALSE)
# LIWC estimates
govt_raw <- liwc_extractor(govt)
# loessed point estimates
govt_results <- liwc_loess(govt_raw)
# get the loess lines for plotting
govt_lines <- loess_lines(govt_raw)
table_for_paper_F <- cbind(FARC[,-1], FARC_results)
table_for_paper_F <- dplyr::select(table_for_paper_F, date, text, EmoNeg, EmoPos)
colnames(table_for_paper_F) <- c("Date", "Text", "Percent neg. emotion", "Percent pos. emotion")
table_for_paper_g <- cbind(govt, govt_results[,-5])
table_for_paper_g <- dplyr::select(table_for_paper_g, date, text, EmoNeg, EmoPos)
table_for_paper_g$Translation <- NA
table_for_paper_g <- table_for_paper_g[,c("date", "text", "Translation", "EmoNeg", "EmoPos")]
colnames(table_for_paper_g) <- c("Date", "Text", "Translation", "Neg", "Pos")
# pick a few that we want
table_for_paper_g_n <- filter(table_for_paper_g, Neg > 28)
table_for_paper_g_n <- arrange(table_for_paper_g_n, desc(Neg))
table_for_paper_g_p <- filter(table_for_paper_g, Pos > 33)
table_for_paper_g_p <- arrange(table_for_paper_g_p, desc(Pos))
View(table_for_paper_F)
table_for_paper_F_n <- filter(table_for_paper_F_n, Neg > 7)
table_for_paper_F_n <- arrange(table_for_paper_F_n, desc(Neg))
table_for_paper_F_p <- filter(table_for_paper_F_p, Pos > 19)
table_for_paper_F_p <- arrange(table_for_paper_F_p, desc(Pos))
table_for_paper_F <- cbind(FARC[,-1], FARC_results)
table_for_paper_F <- dplyr::select(table_for_paper_F, date, text, EmoNeg, EmoPos)
table_for_paper_F$Translation <- NA
table_for_paper_F <- table_for_paper_F[,c("date", "text", "Translation", "EmoNeg", "EmoPos")]
colnames(table_for_paper_F) <- c("Date", "Text", "Translation", "Percent neg. emotion", "Percent pos. emotion")
# now for FARC
table_for_paper_F_n <- filter(table_for_paper_F, Neg > 7)
table_for_paper_F_n <- arrange(table_for_paper_F_n, desc(Neg))
table_for_paper_F_p <- filter(table_for_paper_F, Pos > 19)
table_for_paper_F_p <- arrange(table_for_paper_F_p, desc(Pos))
table_for_paper_F_n <- filter(table_for_paper_F, Neg > 7)
colnames(table_for_paper_F) <- c("Date", "Text", "Translation", "Neg.", "Pos.")
# now for FARC
table_for_paper_F_n <- filter(table_for_paper_F, Neg > 7)
table_for_paper_F_n <- arrange(table_for_paper_F_n, desc(Neg))
table_for_paper_F_p <- filter(table_for_paper_F, Pos > 19)
table_for_paper_F_p <- arrange(table_for_paper_F_p, desc(Pos))
table_for_paper_F_n <- filter(table_for_paper_F, Neg > 7)
colnames(table_for_paper_F) <- c("Date", "Text", "Translation", "Neg", "Pos")
table_for_paper_F_n <- filter(table_for_paper_F, Neg > 7)
table_for_paper_F_n <- arrange(table_for_paper_F_n, desc(Neg))
table_for_paper_F_p <- filter(table_for_paper_F, Pos > 19)
table_for_paper_F_p <- arrange(table_for_paper_F_p, desc(Pos))
stargazer(table_for_paper_F_p, summary = FALSE, digits = 2, title = "Sample of Positive FARC Statements")
stargazer(table_for_paper_F_n, summary = FALSE, digits = 2, title = "Sample of Positive FARC Statements")
View(table_for_paper_F_n)
View(table_for_paper_F_p)
stargazer(table_for_paper_F_n, summary = FALSE, digits = 2, title = "Sample of Negative FARC Statements")
